\documentclass{beamer}
\mode<presentation>
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{advdate}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{listings}
\usepackage{url}
\usepackage[dvipsnames]{xcolor}
\def\UrlBreaks{\do\/\do-}
\usetheme{Madrid}
\usecolortheme{beaver}
\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,right]{author in head/foot}%
    \insertframenumber{} / \inserttotalframenumber\hspace*{2ex} 
  \end{beamercolorbox}}%
  \vskip0pt%
}
\setbeamertemplate{navigation symbols}{}

\providecommand{\nCr}[2]{\,^{#1}C_{#2}} % nCr
\providecommand{\nPr}[2]{\,^{#1}P_{#2}} % nPr
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
%\newcommand{\solution}{\noindent \textbf{Solution: }}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\let\vec\mathbf

\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}

\numberwithin{equation}{section}

\title{Speech Recognition}
\author{Tadipatri Uday Kiran Reddy\\EE19BTECH11038 \\ Dept. of Electrical Engg.,\\IIT Hyderabad.}

%\date{\today} 
\logo{\includegraphics[height=1.5cm]{./figs/logo.jpg}}
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section*{Outline}
\begin{frame}
\tableofcontents
\end{frame}
%\subsection{Literature}
\section{Data Augmentation}
\subsection{Zero Padding}
\begin{frame}
\frametitle{Zero Padding}
This is a technique to increase no of samples of a given audio sample
\begin{description}[font=$\bullet$~\normalfont\scshape\color{red!50!black}]
\item [] Declare an empty array of fixed length which is same length for training the data
\item [] Add the data of the audio sample in the array
\item [] Do this in a circular shift way until desired no of samples are required
\end{description}

\end{frame}

%\subsection{Literature}
\subsection{Feature Extraction}
\begin{frame}
\frametitle{MFCC - Mel Frequency Cepstral Coefficient}
%\framesubtitle{Literature}
 MFCC takes into account human perception for sensitivity at appropriate frequencies by converting the conventional frequency to Mel Scale.\\
 \begin{equation*}
     Mel(f) = 1125\ln({1 + \frac{f}{700}})
 \end{equation*}
 In our case, it returns a matrix of (49X39) i.e,49 time steps and each with 39 features.
\end{frame}

\section{Which Nerural Network??}
\subsection{Feedforward Neural Network}
\begin{frame}
\frametitle{Feedforward Neural Network}
\includegraphics[width=0.5\columnwidth]{./figs/FFNN.jpeg}\\
Here in Feed forward Neural Networks such as MLPN's at time t for a input {$x$(f)}_{t\geq0}\\ output would be {f(w,x(t))}_{t\geq0},{\textbf{without regard to the previous history}}.\\
i.e ,Ouput at t is independent of input at t-1 it only depends on the input at t\\
\textbf{So this kind of neural network is not suitable for Speech recognition}
The main advantage of the neural neural network over others is its memory for processing Temporal signals\\
\end{frame}
\subsection{Recurrent Neural Network}
\begin{frame}
\frametitle{Recurrent Neural Networks}
\includegraphics[width=0.5\columnwidth]{./figs/RNN.png}\\
RNN has an edge over FFNN because of its memory for processing Temporal Signals\\
Here $h_t$ is the hidden state at time step t\\
\begin{equation*}
    {h_t}=f({x_t}{W_{xh}} + {W_{hh}}{h_{t-1}})
\end{equation*}\\
where f is a non-linear function\\
For initial hidden state is initialised with all zeros,i.e,(for {s_{-1}})
\end{frame}
\begin{frame}{Cost Function}
    \begin{equation*}
        {o_t} = softmax(V_{st})
        \end{equation*}
       \begin{equation*}
        softmax{(\vec{z})_i} = \sigma({\vec{z}}) = \frac{e^{z_i}}{\sum_{j=1}^{K}e^{z_j}};\\
        \end{euation*}
       
        for {i = 1,2,3,,,,K} and {\vec{z} = (z_1,z_2,z_3,,,,z_K)}\\
\begin{description}[font=$\bullet$~\normalfont\scshape\color{blue!50!blue}]
\item [] Categorical Cross Entropy
\end{description}
                                \begin{equation*}
        E(y_t,\hat{y_t}) = -{y_t}\log{\hat{y_t}}
                        \end{equation*}
                                                \begin{equation*}
        E(y,\hat{y}) = \sum_{n =1}^{T} E(y_t,\hat{y_t})
    \end{equation*}
    
\end{frame}
\begin{frame}{Optimiser}
We use Root Mean Square Error\\
\begin{equation*}
    {J(\vec{w})} = \frac{1}{m}\frac{1}{T}{\sum_{i=1}^{m}}{\sum_{t=1}^{T}}(y_i(t) -f(\vec{w},{\vec{x}_i}(1:t)))^2
\end{equation*}
Here the error is calclulated based on previous history also\\
\end{frame}
\begin{frame}{Back Propagation}\\
\includegraphics[width=0.5\columnwidth]{./figs/Back.png}\\
\begin{equation*}
    \frac{\partial E}{\partial {W}} = {\sum_{i=0}^{3}}\frac{\partial E_3}{\partial {\hat{y}_3}}\frac{\partial {\hat{y}_3}}{\partial {s_3}}\frac{\partial {s_3}}{\partial {s_k}}\frac{\partial {s_k}}{\partial {W}}
\end{equation*}
    Now we have learn the data with the gradient $\frac{\partial E}{\partial W}$ back along the chain\\
    But the gradient becomes negligible over the chain.\\
    So RNN fails in storing the gradient.
\end{frame}
\subsection{Gated Recurrent Neural Networks}
\subsubsection{LSTM}
\begin{frame}{LSTM - Long short Term Memory}
\includegraphics[width=0.8\columnwidth]{./figs/LSTM.png}\\
These operations are used to allow the LSTM to keep or forget information.\\
It has forget gate which decides whether to hold the information or not.\\
This ensures that learning rate does not decrease down the chain.
\end{frame}
\subsubsection{GRU}
\begin{frame}{GRU - Gated Recurrent Units}
\includegraphics[width=0.5\columnwidth]{./figs/GRU.png}\\
The GRU controls the flow of information like the LSTM unit, but without having to use a memory unit. It just exposes the full hidden content without any control.\\
My assumption is that GRU has only two gates(i.e,\textbf{Reset gate} and \textbf{Update gate} ) and also it has less fewer tensor operation,So this may be more efficient and faster compared to LSTM.
\end{frame}
\end{document}